{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Sequence Models in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1 \n",
    "Convince yourself a RNN is just an FF unfolded in time. Run the NumpyRNN code. Set break-points and compare with what you learned about back-propagation in the previous day.\n",
    "\n",
    "Start by loading data Part-of-speech data and configure it for the exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSJ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Part-of-Speech data \n",
    "from lxmls.readers.pos_corpus import PostagCorpusData\n",
    "data = PostagCorpusData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.numpy_models.rnn import NumpyRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN configuration\n",
    "embedding_size = 50   # Size of word embeddings\n",
    "hidden_size = 20     # size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnns.NumpyRNN(\n",
    "    input_size=data.input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=data.output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = data.batches('train', batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 1:\n",
    "\n",
    "Check gradients using the empirical gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.rnn import rnn_parameter_handlers\n",
    "from lxmls.deep_learning.numpy_models.rnn import cross_entropy_loss, backpropagation\n",
    "batch = data.batches('train', batch_size=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = backpropagation(batch['input'], batch['output'], model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print [x.shape for x in model.parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one parameter from the network\n",
    "layer_index = 0 # 0\n",
    "row = 2\n",
    "column = batch['input'].nonzero()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get functions to get and set values of a particular parameter of the model\n",
    "get_parameter, set_parameter = rnn_parameter_handlers(\n",
    "    layer_index=layer_index,\n",
    "    row=row, \n",
    "    column=column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "scale = 10\n",
    "resolution = 1000\n",
    "perturbations = np.linspace(-scale, scale, resolution)\n",
    "\n",
    "# Get study weight value and loss\n",
    "study_loss = cross_entropy_loss(batch['input'], batch['output'], model.parameters)\n",
    "\n",
    "# Compute the loss when varying the study weight\n",
    "parameters = deepcopy(model.parameters)\n",
    "study_weight = float(get_parameter(parameters))\n",
    "loss_range = []\n",
    "for perturbation in perturbations:\n",
    "    perturbated_parameters = set_parameter(parameters, study_weight + perturbation)\n",
    "    perturbated_loss = cross_entropy_loss(batch['input'], batch['output'], perturbated_parameters)\n",
    "    loss_range.append(perturbated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot empirical\n",
    "weight_range = study_weight + perturbations\n",
    "plt.plot(weight_range, loss_range)\n",
    "plt.plot(study_weight, study_loss, 'xr')\n",
    "# Plot real\n",
    "h = plt.plot(\n",
    "    weight_range,\n",
    "    get_parameter(gradient)*(weight_range - study_weight) + study_loss, \n",
    "    'r--'\n",
    ")\n",
    "plt.title('layer %d parameter (%d, %d)' % (layer_index, row, column))\n",
    "plt.xlabel('parameter value')\n",
    "plt.ylabel('loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
