{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.readers.sentiment_reader as srs\n",
    "from lxmls.deep_learning.utils import AmazonData\n",
    "corpus = srs.SentimentCorpus(\"books\")\n",
    "data = AmazonData(corpus=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 Implement Backpropagation for an MLP in Numpy and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "geometry = [corpus.nr_features, 20, 2]\n",
    "activation_functions = ['sigmoid', 'softmax']\n",
    "\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.numpy_models.mlp import NumpyMLP\n",
    "model = NumpyMLP(\n",
    "    geometry=geometry,\n",
    "    activation_functions=activation_functions,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 1:\n",
    "Check gradients using the empirical gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.mlp import mlp_parameter_handlers\n",
    "#from lxmls.deep_learning.numpy_models.mlp import cross_entropy_loss, backpropagation\n",
    "batch = data.batches('train', batch_size=batch_size)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = model.backpropagation(batch['input'], batch['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss values for the study weight and perturbed versions. Take into account that the gradient for the first layer (embeddings) will be zero unless the word is in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one parameter from the network\n",
    "layer_index = 1 # 0\n",
    "is_bias = 0\n",
    "row = 1\n",
    "column = 1 #batch['input'].nonzero()[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get functions to get and set values of a particular parameter of the model\n",
    "get_parameter, set_parameter = mlp_parameter_handlers(\n",
    "    layer_index=layer_index,\n",
    "    is_bias=is_bias,\n",
    "    row=row, \n",
    "    column=column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "perturbations = np.linspace(-10, 10, 200)\n",
    "\n",
    "# Get study weight value and loss\n",
    "study_loss = model.cross_entropy_loss(batch['input'], batch['output'])\n",
    "\n",
    "# Compute the loss when varying the study weight\n",
    "parameters = deepcopy(model.parameters)\n",
    "study_weight = float(get_parameter(parameters))\n",
    "loss_range = []\n",
    "old_parameters = list(model.parameters)\n",
    "for perturbation in perturbations: \n",
    "    model.parameters = set_parameter(parameters, study_weight + perturbation)\n",
    "    perturbated_loss = model.cross_entropy_loss(batch['input'], batch['output'])\n",
    "    loss_range.append(perturbated_loss)\n",
    "model.parameters = old_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot empirical\n",
    "weight_range = study_weight + perturbations\n",
    "plt.plot(weight_range, loss_range)\n",
    "plt.plot(study_weight, study_loss, 'xr')\n",
    "# Plot real\n",
    "h = plt.plot(\n",
    "    weight_range,\n",
    "    get_parameter(gradient)*(weight_range - study_weight) + study_loss, \n",
    "    'r--'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 2:\n",
    "Train a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "\n",
    "    # Inform user\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
